#!/usr/bin/env python
# coding: utf-8

# <h1> Importing Libraries and Exploring Data

# In[29]:


import pandas as pd
import numpy as np
import warnings
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from wordcloud import WordCloud
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')
import sys


# In[30]:


print(sys.executable)


# <h2> Input 

# In[4]:


#Reading in the files
df = pd.read_json('/Users/pinakshome/Downloads/pii-detection/train.json')


# <h3> Pre-EDA

# In[5]:


df.head()


# In[6]:


df.shape


# In[7]:


df.dtypes


# In[31]:


(df['trailing_whitespace'][1][:5])


# In[21]:


df['labels'][1][:5]


# In[22]:


len(df['tokens'][1])


# In[24]:


df['tokens'][1][:8]


# In[32]:


len(df['full_text'])


# In[33]:


len(df['full_text'][1])


# <H3> EDA on full_text feature

# <h3> Text Analysis

# In[34]:


df['text_length'] = df['full_text'].apply(len)
plt.hist(df['text_length'], bins=10)
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.title('Distribution of Text Length')
plt.show()

#Checking the distribution of text length


# In[35]:


###Checking the top 20 words across all documents not considering stop words

stop_words = set(stopwords.words('english'))
all_words = [word for tokens in df['tokens'] for word in tokens if word.lower() not in stop_words]
word_freq = nltk.FreqDist(all_words)
word_freq.plot(20, title="Top 20 Words (excluding stopwords)")


# <h3> Tokens

# In[36]:


df['token_count'] = df['tokens'].apply(len)
plt.hist(df['token_count'], bins=10)
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.title('Distribution of Token Count')
plt.show()


# In[37]:


unique_tokens = set(all_words)
print(f"Number of unique tokens: {len(unique_tokens)}")


# In[40]:


### Identifying if tokens are already standardized 

from collections import defaultdict


all_tokens = [token for sublist in df['tokens'] for token in sublist]

# Creating a dictionary to hold each unique token and its variations
token_variations = defaultdict(set)

# Populating the dictionary
for token in all_tokens:
    # Using a standardized form for matching 
    standardized_token = token.lower()
    token_variations[standardized_token].add(token)

# Identifying tokens with variations
count=0
for standard_token, variations in token_variations.items():
    count+=1
    if len(variations) > 1 and count<50:
        print(f"Token: {standard_token}, Variations: {variations}")
        
        
        
        
####### The tokens are not completely inconsistency free

## We will check further to see if we have punctuational token variations


# In[60]:


import string
from collections import defaultdict


all_tokens = [token for sublist in df['tokens'] for token in sublist]

# Function to remove punctuation from a token
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

# Dictionary to keep track of tokens with and without punctuation
tokens_with_without_punct = defaultdict(lambda: {'with': set(), 'without': set()})

for token in all_tokens:
    stripped_token = remove_punctuation(token)
    if stripped_token != token:
        # The token had punctuation and was modified
        tokens_with_without_punct[stripped_token]['with'].add(token)
    else:
        # The token had no punctuation to remove
        tokens_with_without_punct[stripped_token]['without'].add(token)

# Now, we'll find tokens that exist both with and without punctuation
tokens_with_both_forms = {token: forms for token, forms in tokens_with_without_punct.items() if forms['with'] and forms['without']}

# Display results


count=0

for token, forms in tokens_with_both_forms.items():
    print(f"Base Token: {token}, With Punctuation: {forms['with']}, Without Punctuation: {forms['without']}")
    count+=1
    if count==10:
        break


# ### Based on the cell results above it seems like there exists token with punctuation versions
# 
# - Considering standardizing the tokens after a more basic EDA of the rest of the features

# <h3> Label Distribution

# In[44]:


all_labels = [label for sublist in df['labels'] for label in sublist]
label_freq = nltk.FreqDist(all_labels)
plt.xticks(rotation=90)
plt.bar(label_freq.keys(), label_freq.values())
plt.xlabel('Labels')
plt.ylabel('Frequency')
plt.title('Distribution of Labels')
plt.show()


# In[50]:


all_labels = [label for sublist in df['labels'] for label in sublist if label != "O"]

#  Count the frequencies of the worded labels
label_frequencies = Counter(all_labels)

#  Print the frequency of each label
for label, count in label_frequencies.items():
    print(f"{label}: {count}")


# In[51]:


### Checking out some rows with these labels present 
labels_of_interest = ["B-EMAIL", "B-ID_NUM"]

for label in labels_of_interest:
    # Use a boolean mask to filter rows where our label of interest is found
    mask = df['labels'].apply(lambda x: label in x)
    
    # Display the filtered rows for this label
    display(df[mask].head())  # Using .head() to display a few rows, adjust as needed


# <h4> Wordcloud for Text

# In[53]:


wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(' '.join(all_words))
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 
plt.show()


# ### First Attempt : For the first attempt, we will not be adding addtional datasets. We'll try and optimize the exisiting features through EDA and applicable transformations before deciding on the model 

# ### This alligns with a Named Entity Recognition (NER) task

# #### For label observations we know that the ratio of '0' in the label lists are way higher than the worded labels. We'll print out some of the document rows with the worded labels, which will help get an overview of what PII exists in the text of a document row

# In[59]:


def print_labels_with_values(df):
    for index, row in df.iterrows():
        print(f"Document number {row['document']}:")
        
        # Zip together the tokens and labels for parallel iteration
        paired = list(zip(row['tokens'], row['labels']))
        
        # Filter out pairs where label is 'O', and print remaining tokens with their labels
        for token, label in paired:
            if label != 'O':
                print(f"\tToken: {token}, Label: {label}")
        print("-" * 50)  # Separator for readability

print_labels_with_values(df[300:350])






